{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Machine_Learning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFdA4pcgxZfn",
        "colab_type": "text"
      },
      "source": [
        "# **Word2Vec using Gensim**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7WXjmUR81vr",
        "colab_type": "code",
        "outputId": "d3bcdd70-4c01-488c-d4d7-3cfd150dbbba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlHekasr8DDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python program to generate word vectors using Word2Vec \n",
        "\n",
        "# importing all necessary modules \n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import warnings \n",
        "\n",
        "warnings.filterwarnings(action = 'ignore') \n",
        "\n",
        "import gensim \n",
        "from gensim.models import Word2Vec "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FIi_W478ZW5",
        "colab_type": "code",
        "outputId": "d9cc1807-bc20-4883-84db-2fa8bec39251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Reads ‘data.txt’ file \n",
        "sample = open(\"write.txt\", \"r\") \n",
        "s = sample.read() \n",
        "s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GeeksforGeeks\\nCustom Search\\n\\nLoginCOURSES HIRE WITH US Skip to content\\n Algo ▼ DS ▼ Languages ▼ Interview ▼ Students ▼ GATE ▼ CS Subjects ▼ Quizzes ▼ GBlog Puzzles Practice\\n▲\\nPython | Pandas dataframe.replace()\\nPython | Pandas dataframe.replace()\\nWhy is Python the Best-Suited Programming Language for Machine Learning?\\nHow to Start Learning Machine Learning?\\n12 Reasons Why You Should Learn Python in 2019\\nPython | Django News App\\nPython | Speech recognition on large audio files\\nML | Training Image Classifier using Tensorflow Object Detection API\\nHandwritten Equation Solver in Python\\nInfyTQ 2019 : Find the position from where the parenthesis is not balanced\\nHow to Become a Data Scientist in 2019: A Complete Guide\\nJavaScript vs Python : Can Python Overtop JavaScript by 2020?\\nPython | Get all substrings of given string\\nPython | Extract digits from given string\\nPython | os.path.join() method\\nHow to approach a Machine Learning project : A step-wise guidance\\nPython | Ways to find all permutation of a string\\nInvisible Cloak using OpenCV | Python Project\\nCheck if email address valid or not in Python\\n30 minutes to machine learning\\nHow to Become a Data Analyst in 2019: A Complete Guide\\nPython | Count overlapping substring in a given string\\nImplementing Apriori algorithm in Python\\nML | Rainfall prediction using Linear regression\\nBest Books to Learn Python for Beginners and Experts in 2019\\nPython | Linear Regression using sklearn\\nML | One Hot Encoding of datasets in Python\\nPython | Convert a list to dictionary\\nRandom Forest Regression in Python\\nPython - Fastest Growing Programming Language\\n\\nperm_identity\\nPython | Pandas dataframe.replace()\\nPython is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric python packages. Pandas is one of those packages and makes importing and analyzing data much easier.\\n\\nPandas dataframe.replace() function is used to replace a string, regex, list, dictionary, series, number etc. from a dataframe. This is a very rich function as it has many variations.\\nThe most powerful thing about this function is that it can work with Python regex (regular expressions).\\n\\nSyntax: DataFrame.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method=’pad’, axis=None)\\n\\n\\n\\nParameters:\\nto_replace : [str, regex, list, dict, Series, numeric, or None] pattern that we are trying to replace in dataframe.\\nvalue : Value to use to fill holes (e.g. 0), alternately a dict of values specifying which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.\\ninplace : If True, in place. Note: this will modify any other views on this object (e.g. a column from a DataFrame). Returns the caller if this is True.\\nlimit : Maximum size gap to forward or backward fill\\nregex : Whether to interpret to_replace and/or value as regular expressions. If this is True then to_replace must be a string. Otherwise, to_replace must be None because this parameter will be interpreted as a regular expression or a list, dict, or array of regular expressions.\\nmethod : Method to use when for replacement, when to_replace is a list.\\n\\nReturns: filled : NDFrame\\n\\nFor link to CSV file Used in Code, click here\\n\\nExample #1: Replace team “Boston Celtics” with “Omega Warrior” in the nba.csv file\\n\\nfilter_none\\nbrightness_4\\n# importing pandas as pd \\nimport pandas as pd \\n  \\n# Making data frame from the csv file \\ndf = pd.read_csv(\"nba.csv\") \\n  \\n# Printing the first 10 rows of the data frame for visualization \\ndf[:10] \\nOutput:\\n\\n\\n\\nWe are going to replace team “Boston Celtics” with “Omega Warrior” in the ‘df’ data frame\\n\\nfilter_none\\nbrightness_4\\n# this will replace \"Boston Celtics\" with \"Omega Warrior\" \\ndf.replace(to_replace =\"Boston Celtics\", \\n                 value =\"Omega Warrior\") \\nOutput:\\n\\n\\n\\n \\n\\nExample #2: Replacing more than one value at a time. Using python list as an argument\\n\\nWe are going to replace team “Boston Celtics” and “Texas” with “Omega Warrior” in the ‘df’ dataframe.\\n\\nfilter_none\\nbrightness_4\\n# importing pandas as pd \\nimport pandas as pd \\n  \\n# Making data frame from the csv file \\ndf = pd.read_csv(\"nba.csv\") \\n  \\n# this will replace \"Boston Celtics\" and \"Texas\" with \"Omega Warrior\" \\ndf.replace(to_replace =[\"Boston Celtics\", \"Texas\"],  \\n                            value =\"Omega Warrior\") \\nOutput:\\n\\nNotice the College column in the first row, “Texas” has been replaced with “Omega Warriors”\\n \\nExample #3: Replace the Nan value in the data frame with -99999 value.\\n\\nfilter_none\\nbrightness_4\\n# importing pandas as pd \\nimport pandas as pd \\n  \\n# Making data frame from the csv file \\ndf = pd.read_csv(\"nba.csv\") \\n  \\n# will replace  Nan value in dataframe with value -99999  \\ndf.replace(to_replace = np.nan, value =-99999) \\nOutput:\\n\\nNotice all the Nan value in the data frame has been replaced by -99999. Though for practical purposes we should be careful with what value we are replacing nan value.\\n\\n\\n\\nRecommended Posts:\\nPython | pandas.map()\\nPython | Pandas Series.sem()\\nPython | Pandas dataframe.all()\\nPython | Pandas TimedeltaIndex.contains\\nPython | Pandas dataframe.eq()\\nPython | Pandas dataframe.pow()\\nPython | Pandas Series.gt()\\nPython | Pandas Series.dt.tz\\nPython | Pandas Series.min()\\nPython | Pandas TimedeltaIndex.name\\nPython | Pandas dataframe.std()\\nPython | Pandas dataframe.cov()\\nPython | Pandas Timestamp.now\\nPython | Pandas dataframe.sem()\\nPython | Pandas Series.all()\\n\\nShubham__Ranjan\\nCheck out this Author\\'s contributed articles.\\nIf you like GeeksforGeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. See your article appearing on the GeeksforGeeks main page and help other Geeks.\\n\\nPlease Improve this article if you find anything incorrect by clicking on the \"Improve Article\" button below.\\n\\n\\n\\n\\nArticle Tags : PythonPython pandas-dataFramePython pandas-dataFrame-methodsPython-pandasTechnical Scripter 2018\\nthumb_up\\nBe the First to upvote.\\n\\n\\n \\n0\\n\\nNo votes yet.\\nFeedback/ Suggest ImprovementAdd NotesImprove Article  \\nPlease write to us at contribute@geeksforgeeks.org to report any issue with the above content.\\nPost navigation\\nPrevious\\nfirst_page Python | Pandas DataFrame.abs()\\nNext\\nlast_pagenumpy.nancumsum() in Python\\n\\n\\n\\n\\nWriting code in comment? Please use ide.geeksforgeeks.org, generate link and share the link here.\\n\\n\\nLoad Comments\\n\\nauto\\nMost popular in Python\\nPython | Split CamelCase string to individual strings\\nPython | Numpy matrix.sum()\\nPython | Numpy matrix.std()\\nPython | Numpy matrix.sort()\\nPython program to convert hex string to decimal\\n\\nMore related articles in Python\\nPython | Add leading Zeros to string\\nPython | Add trailing Zeros to string\\nPython| AnchorLayout in Kivy\\nPython | AnchorLayout in Kivy using .kv file\\nPython | StackLayout in Kivy\\n\\n\\n✍\\nWrite a Testimonial\\n⇣\\nGeeksforGeeks\\n5th Floor, A-118,\\nSector-136, Noida, Uttar Pradesh - 201305\\nfeedback@geeksforgeeks.org\\nCOMPANY\\nAbout Us\\nCareers\\nPrivacy Policy\\nContact Us\\nLEARN\\nAlgorithms\\nData Structures\\nLanguages\\nCS Subjects\\nVideo Tutorials\\nPRACTICE\\nCourses\\nCompany-wise\\nTopic-wise\\nHow to begin?\\nCONTRIBUTE\\nWrite an Article\\nWrite Interview Experience\\nInternships\\nVideos\\n    \\n@geeksforgeeks, Some rights reserved\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R10N9Iqz8v8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replaces escape character with space \n",
        "f = s.replace(\"\\n\", \" \") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKrcXmpAxQ9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [] \n",
        "\n",
        "# iterate through each sentence in the file \n",
        "for i in sent_tokenize(f): \n",
        "\ttemp = [] \n",
        "\t\n",
        "\t# tokenize the sentence into words \n",
        "\tfor j in word_tokenize(i): \n",
        "\t\ttemp.append(j.lower()) \n",
        "\n",
        "\tdata.append(temp) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GXlTJU0xTKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create CBOW model \n",
        "cbow = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5) \n",
        "\n",
        "# Create Skip Gram model \n",
        "skipgram = gensim.models.Word2Vec(data, min_count = 1, size = 100,  window = 5, sg = 1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGZcTm-exUt-",
        "colab_type": "code",
        "outputId": "d532e6dc-dbe2-4fda-eb98-3438b5cd20ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Print results \n",
        "print(\"machine', 'learning\", \n",
        "\tcbow.similarity('machine', 'learning')) \n",
        "\n",
        "# Print results \n",
        "print(\"machine', 'learning\", \n",
        "\tskipgram.similarity('machine', 'learning')) \n",
        "  \n",
        "print(\"deep', 'learning'\", \n",
        "\tcbow.similarity('python', 'pandas')) \n",
        "\n",
        "print(\"deep', 'learning'\", \n",
        "\tskipgram.similarity('python', 'pandas')) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "machine', 'learning -0.10583428\n",
            "machine', 'learning 0.15000153\n",
            "deep', 'learning' 0.057878252\n",
            "deep', 'learning' 0.8225623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unuDpse-07gX",
        "colab_type": "code",
        "outputId": "9351dbf4-c706-44f4-cc1a-fe5a7c1d2d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "cbow.wv['machine']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.37937644e-03, -8.27057462e-04,  1.74844207e-03,  3.91346915e-03,\n",
              "       -4.06864937e-03, -3.42112314e-03, -2.81069166e-04, -1.25445367e-03,\n",
              "       -1.73640007e-03,  2.68886238e-03, -1.38437806e-03, -3.18228779e-03,\n",
              "        4.21796180e-03, -1.71508480e-04,  2.24784118e-04, -5.94904879e-04,\n",
              "       -5.87752089e-04, -2.93880352e-03,  2.49529607e-03,  4.17589163e-03,\n",
              "       -3.18101561e-03, -2.93463096e-03, -3.97329917e-03,  3.24379606e-03,\n",
              "       -1.82258361e-03, -4.35636332e-03, -2.01369426e-03, -4.26478544e-03,\n",
              "       -1.74441317e-03,  8.14897299e-04,  4.32861829e-03, -2.94423243e-03,\n",
              "       -3.06681870e-03, -3.04108090e-03, -2.12622690e-03, -1.59289211e-03,\n",
              "        3.18719284e-03,  2.73641362e-03, -1.20579242e-03,  4.73455247e-03,\n",
              "        4.07070713e-03,  3.10488953e-03,  1.66692573e-03, -3.07713082e-04,\n",
              "       -8.85984860e-04,  1.27593125e-03,  4.06606263e-03,  2.41512689e-03,\n",
              "        1.58953085e-03,  2.58777826e-03,  2.43254914e-03, -1.73875981e-03,\n",
              "        1.63368497e-03, -4.70772432e-03, -4.74134972e-03,  2.75033596e-03,\n",
              "        2.08074972e-03, -5.47699274e-05,  4.09878790e-03,  3.69550427e-03,\n",
              "       -3.07559432e-03,  4.36220458e-03, -1.58434163e-03, -9.80625162e-04,\n",
              "        1.81629357e-03, -2.55578104e-03, -2.16568797e-03,  3.15321260e-03,\n",
              "        4.96151950e-03, -3.77516332e-03,  3.49246035e-03, -1.94423588e-03,\n",
              "       -1.45619619e-03, -1.46576692e-03,  9.24702268e-04, -7.71073333e-04,\n",
              "        1.01468631e-03,  2.97555613e-04, -2.05021701e-03, -6.72848255e-05,\n",
              "       -2.65476806e-03, -4.38064337e-03, -3.81547399e-03, -1.88338140e-03,\n",
              "        1.47109374e-03, -2.80178711e-03,  4.66297800e-03,  2.42723222e-03,\n",
              "        2.56480742e-03, -5.66979637e-04,  1.10035995e-04,  4.65045543e-03,\n",
              "        4.50408552e-03,  4.47054533e-03, -4.79937019e-03, -4.57672495e-03,\n",
              "       -9.17463447e-04, -2.04855343e-03, -1.42866874e-03,  1.56187173e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJLSjA4E0_N3",
        "colab_type": "code",
        "outputId": "db90711c-e2d5-428d-ade9-d537624b31ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "skipgram.wv.most_similar('python')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('|', 0.8651377558708191),\n",
              " (')', 0.85173499584198),\n",
              " ('to', 0.8393042683601379),\n",
              " ('(', 0.8270609974861145),\n",
              " ('the', 0.824999213218689),\n",
              " ('pandas', 0.8225623369216919),\n",
              " (',', 0.798292338848114),\n",
              " ('in', 0.7843585014343262),\n",
              " ('string', 0.7821933031082153),\n",
              " ('with', 0.7725687026977539)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLOX76YMxjFX",
        "colab_type": "text"
      },
      "source": [
        "# **Sentiment analysis using Textblob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYjO4ue5xnIs",
        "colab_type": "code",
        "outputId": "52c73256-419e-407d-c08d-249ca0935af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "#nltk.download('brown')\n",
        "#nltk.download('punkt')\n",
        "from textblob import TextBlob"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bii7aZVy4qX",
        "colab_type": "code",
        "outputId": "e1f95d83-8065-4856-f436-49da71b1556f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "wiki = TextBlob(\"Python is an  high-level, general-purpose programming language.\")\n",
        "print(wiki.tags)\n",
        "print(wiki.noun_phrases)\n",
        "print(wiki.sentiment)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('3aslema', 'CD'), ('ena', 'JJ'), ('ismi', 'NNS'), ('elyes', 'NNS')]\n",
            "['3aslema ena ismi elyes']\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njlx2cK5y93P",
        "colab_type": "code",
        "outputId": "0667142f-fdfd-4645-cfab-e0a4962fe375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "zen = TextBlob(\"Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex.\")\n",
        "print(zen.words)\n",
        "print(zen.sentences)\n",
        "print('')\n",
        "for sentence in zen.sentences:\n",
        "  print(sentence.sentiment)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Beautiful', 'is', 'worse', 'than', 'ugly', 'Explicit', 'is', 'better', 'than', 'implicit', 'Simple', 'is', 'better', 'than', 'complex']\n",
            "[Sentence(\"Beautiful is worse than ugly.\"), Sentence(\"Explicit is better than implicit.\"), Sentence(\"Simple is better than complex.\")]\n",
            "\n",
            "Sentiment(polarity=-0.08333333333333333, subjectivity=0.8666666666666667)\n",
            "Sentiment(polarity=0.5, subjectivity=0.5)\n",
            "Sentiment(polarity=0.06666666666666667, subjectivity=0.41904761904761906)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FITqOiLyRTp",
        "colab_type": "text"
      },
      "source": [
        "# **Named Entity Recognition using Spacy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH9Glu1ByWU2",
        "colab_type": "code",
        "outputId": "47b69374-9761-411a-9d7d-f6db094e94ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "#pip install spacy\n",
        "#python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process whole documents\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
        "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Analyze syntax\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verbs: ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'be', 'talk', 'say']\n",
            "Sebastian Thrun PERSON\n",
            "Google ORG\n",
            "2007 DATE\n",
            "American NORP\n",
            "Thrun PERSON\n",
            "Recode ORG\n",
            "earlier this week DATE\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}